[
  {
    "objectID": "research_blog.html",
    "href": "research_blog.html",
    "title": "The Alchemist’s Workshop",
    "section": "",
    "text": "GPT from scratch\n\n\n\n\n\n\n\n\nMar 10, 2023\n\n\nSuchith Prabhu\n\n\n\n\n\n\n\n\n\n\n\n\nBewakoof Detector\n\n\n\n\n\n\n\n\nMar 4, 2023\n\n\nSuchith Prabhu\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Home",
      "Suchith Prabhu",
      "Blogs",
      "Research"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Suchith Prabhu",
    "section": "",
    "text": "I am a 5th-year Ph.D. scholar under the supervision of Dr. Manik Varma and Dr. Sumeet Agarwal at School of AI, IIT Delhi. My research domain is Extreme Classification, simply put classification on steroids.\nTennis is my first love—yes, an engineer’s life—and someday I hope to play in professional tournaments. I would also love to grow as a musician, but given the current demands of Ph.D. life, that feels like a far-fetched dream—for now.",
    "crumbs": [
      "Home",
      "Suchith Prabhu",
      "About"
    ]
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Suchith Prabhu",
    "section": "Education",
    "text": "Education\nIndian Institute of Technology Delhi | New Delhi, India  PhD in Artificial Intelligence | Aug 2021 - June 2026\nNational Institute of Technology Goa | Goa, India  B.Tech in Computer Science | Aug 2015 - May 2019",
    "crumbs": [
      "Home",
      "Suchith Prabhu",
      "About"
    ]
  },
  {
    "objectID": "about.html#publications",
    "href": "about.html#publications",
    "title": "Suchith Prabhu",
    "section": "Publications",
    "text": "Publications\n\nGraph Regularized Encoder Training for Extreme Classification Anshul Mittal, Shikhar Mohan, Deepak Saini, Siddarth Asokan, Suchith Chidananda Prabhu, Lakshya Kumar, Pankaj Malhotra, Jian Jiao, Amit Singh, Sumeet Agarwal, Soumen Chakrabarti, Purushottam Kar, Manik Varma. Companion Proceedings of the ACM on Web Conference 2025, Sydney, Australia. DOI: 10.1145/3701716.3715230\nMOGIC: Metadata-infused Oracle Guidance for Improved Extreme Classification Suchith Chidananda Prabhu, Bhavyajeet Singh, Anshul Mittal, Siddarth Asokan, Shikhar Mohan, Deepak Saini, Yashoteja Prabhu, Lakshya Kumar, Jian Jiao, Amit S, Niket Tandon, Manish Gupta, Sumeet Agarwal, Manik Varma. Forty-second International Conference on Machine Learning, 2025 Paper link",
    "crumbs": [
      "Home",
      "Suchith Prabhu",
      "About"
    ]
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Suchith Prabhu",
    "section": "Experience",
    "text": "Experience\nVehant Technologies | Software Developer | July 2019 - July 2021",
    "crumbs": [
      "Home",
      "Suchith Prabhu",
      "About"
    ]
  },
  {
    "objectID": "posts/research/bewakoof-detector/index.html",
    "href": "posts/research/bewakoof-detector/index.html",
    "title": "Bewakoof Detector",
    "section": "",
    "text": "I trained this cool image classifier using the fastai library to mess with my friend who we both agree is bewakoof with directions.\nimport ssl\nssl._create_default_https_context = ssl._create_unverified_context\n\n!unset http_proxy\n!unset https_proxy\nThe provided code snippet aims to resolve potential internet connectivity issues that may arise when using an internal network with a proxy. code snippet above is for bypassing internet issue that I got when using a internal network with proxy.\nWe import the fastai library, which includes all the essential functions necessary to create the classifier.\nfrom fastai.vision.all import *"
  },
  {
    "objectID": "posts/research/bewakoof-detector/index.html#data-cleaning",
    "href": "posts/research/bewakoof-detector/index.html#data-cleaning",
    "title": "Bewakoof Detector",
    "section": "Data cleaning",
    "text": "Data cleaning\nI wrote a custom code to create a widget that could rotate and save the manually cropped images since they were not saved correctly, and a few of them were rotated by some degrees.\n\n\nCode\nfrom ipywidgets import widgets\nfrom IPython.display import display\n\n\nfiles = get_image_files(PATH)\n\nbutton_next = widgets.Button(description=\"Next\")\nbutton_pre = widgets.Button(description=\"Previous\")\nbutton_rotate_left = widgets.Button(description=\"Rotate left\", icon=\"rotate-left\")\nbutton_rotate_right = widgets.Button(description=\"Rotate right\", icon=\"rotate-right\")\n\noutput = widgets.Output()\n\nCOUNTER = -1\nIMAGE = None\nMODIFIED = False\n\ndef button_next_eventhandler(obj):\n    global IMAGE, COUNTER, MODIFIED\n    \n    if MODIFIED:\n        IMAGE.save(files[COUNTER])\n        MODIFIED = False\n\n    COUNTER += 1\n    output.clear_output()\n    \n    if COUNTER &lt; len(files):\n        IMAGE = Image.open(files[COUNTER])\n        with output:\n            display(IMAGE.to_thumb(256, 256))\n    else:\n        with output:\n            display(\"ERROR::Buffer overflow.\")\n            \ndef button_rotate_left_eventhandler(obj):\n    global IMAGE, COUNTER, MODIFIED\n    \n    output.clear_output()\n    \n    if COUNTER &gt; -1 and COUNTER &lt; len(files):\n        MODIFIED = True\n        IMAGE = IMAGE.rotate(90)\n        with output:\n            display(IMAGE.to_thumb(256, 256))\n    else:\n        with output:\n            display(\"ERROR::Invalid counter value.\")\n                   \ndef button_rotate_right_eventhandler(obj):\n    global IMAGE, COUNTER, MODIFIED\n    \n    output.clear_output()\n    \n    if COUNTER &gt; -1 and COUNTER &lt; len(files):\n        MODIFIED = True\n        IMAGE = IMAGE.rotate(-90)\n        with output:\n            display(IMAGE.to_thumb(256, 256))\n    else:\n        with output:\n            display(\"ERROR::Invalid counter value.\")\n                     \ndef button_previous_eventhandler(obj):\n    global IMAGE, COUNTER, MODIFIED\n    \n    if MODIFIED:\n        IMAGE.save(files[COUNTER])\n        MODIFIED = False\n    \n    COUNTER -= 1\n    output.clear_output()\n    \n    if COUNTER &gt; -1:\n        IMAGE = Image.open(files[COUNTER])\n        with output:\n            display(IMAGE.to_thumb(256, 256))\n    else:\n        with output:\n            display(\"ERROR::Buffer underflow.\")\n            \nbutton_rotate_left.on_click(button_rotate_left_eventhandler)\nbutton_rotate_right.on_click(button_rotate_right_eventhandler)\nbutton_next.on_click(button_next_eventhandler)\nbutton_pre.on_click(button_previous_eventhandler)\n            \n\nitem_layout = widgets.Layout(margin=\"0 0 50px 0\")\n\nbuttons = widgets.HBox([button_rotate_left, button_rotate_right, button_next, button_pre], layout=item_layout)\n\ntab = widgets.Tab([output])\ntab.set_title(0, 'Image')\n\ndashboard = widgets.VBox([buttons, tab], layout=item_layout)\n\n\n\ndisplay(dashboard)"
  },
  {
    "objectID": "posts/research/scratchGPT/index.html",
    "href": "posts/research/scratchGPT/index.html",
    "title": "GPT from scratch",
    "section": "",
    "text": "Language models have become increasingly popular in recent times. People are using it for various purposes, including information retrieval, language assistance and code generation. These models are very powerful and represent an engineering marvel of the century. We have come a long way from research projects to something that people could use in their daily lives. The basic building blocks and idea behind these models are simple, and with libraries like Pytorch, coding them from scratch would not take much time. It is a straightforward concept that has been scaled to such a massive degree that it performs exceptionally well. In this post, we will go through the steps to building GPT, which is a decoder-only transformer architecture. Throughout this process, we will see that it is not much different from the Bigram language model, with the addition of a few communication and computation layers. While going through this post, you might feel like GPT is based on straightforward ideas that you could have come up with, but it is the result of a decade of rigorous empirical work that has made it a reality.\nPrior to delving into transformer architecture, it would be advantageous to begin with the bigram model and subsequently extend the concept with new architectural techniques.\nThe following code will aid in importing the required modules. We will only be utilizing essential functions from these modules to construct the model from the ground up.\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F"
  },
  {
    "objectID": "posts/research/scratchGPT/index.html#bigram-language-model",
    "href": "posts/research/scratchGPT/index.html#bigram-language-model",
    "title": "GPT from scratch",
    "section": "Bigram Language Model",
    "text": "Bigram Language Model\nWe construct an embedding for each character in the vocabulary with the same dimension as the size of the vocabulary. Each entry denotes the unnormalized log-probabilities of the next character given the current character. We model this such that we minimize the cross-entropy loss to learn this probability distribution. The diagram below provides a better explanation of this process.\n\n\n\n\n\n\n\n\nFigure 1: Bigram Model\n\n\n\n\n\nCode for the Bigram language model.\n\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super(BigramLanguageModel, self).__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, x, y=None):\n        logits = self.token_embedding_table(x) # (B,T,C)\n\n        if y is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            loss = F.cross_entropy(logits.view(B*T, C), y.view(B*T))\n\n        return logits, loss\n\n    @torch.no_grad()\n    def generate(self, idx, max_new_tokens):\n\n        for _ in range(max_new_tokens):\n            logits, _ = self(idx)\n            logits = logits[:, -1, :]\n            probs = F.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n            idx = torch.cat((idx, idx_next), dim=-1)\n        return idx\n\nTo generate a new sequence from the trained model, we sample characters from the learnt distribution.\nWe have created the BigramLanguageModel object below.\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel = BigramLanguageModel(vocab_size)\nmodel = model.to(device)\n\nHere is the text generated by the model before training.\n\n\n\nU3x\nUFcbptwyzsjOLyg3b.vBQC;TdZMxr?q KPe&:MMEhOotlJI.'gv?lq\nFNzXFGklQuRkftks-3bop&g.IIe-lkTDZ$zC;wTbApsBjDKRU.i;XlaOVmujKoWmn 3IFSwVxKM\njhIcITo\nYqlXhauIREGcjN;avU3blg,?b$-;fIGKropZjhGOxSn npFYX!n;v;F FtE?cnhfziNCi3T!\nLuhDXnpm,MH& X'ROuDLxguACkEppAkX!MvAhxSI&gF! vuauPqijhxrqJyS$z-3VmVmB,CRXwWl'Tn cgnVN $VAWgU:a&RseCjvW,BcbSjJPzZl\njr'kO byOr.bDaMLJxcoyl\n3hPulOyOd\n3taQAYcl&tvPcsi&JPE!Sr,-GVkNxBqHKJPAk&GSDOXVE,rA:aGt\nFS,l'aLhyUa:HuRc Jp,pqzXWH,h:r?OLoJ!iYJPcMKGfNPnHMMllxHPEjVkY-3TdZQJmuLvuv;stHZdwsrt\n\n\nThe generated text looks like gibberish and does not resemble human writing. The words are very long, as if the characters have been randomly sampled. Let’s see how the text looks after training.\nBelow is the code for training the model.\n\n# hyperparameters\nmax_iters = 3000\neval_interval = 300\nlearning_rate = 1e-2\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor i in range(max_iters):\n    if i % eval_interval == 0:\n        losses = estimate_loss()\n        print(f\"step {i:04d}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    xb, yb = get_batch(\"train\")\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nstep 0000: train loss 4.6909, val loss 4.6764\nstep 0300: train loss 2.8011, val loss 2.8099\nstep 0600: train loss 2.5521, val loss 2.5599\nstep 0900: train loss 2.5008, val loss 2.5132\nstep 1200: train loss 2.4754, val loss 2.5033\nstep 1500: train loss 2.4693, val loss 2.5059\nstep 1800: train loss 2.4643, val loss 2.4828\nstep 2100: train loss 2.4703, val loss 2.4907\nstep 2400: train loss 2.4656, val loss 2.4919\nstep 2700: train loss 2.4575, val loss 2.4904\n\n\nThis is the text generated by the model after training the Bigram model.\n\n\n\nWhy th T:\nTonof ne h furst w:\nAn h, gie choid welleintrs a wawis LARKETomug!\n\nI welom fohe! da RD:\nULindeye tor, our barand bea nt, t brkifaind asithald So het\nD:\nTham o d! Hist.\nI okngorde andixis hlimends berseiayord Ansthurdous sowhays mecctoncereltougan!'CAnd\nThalou,\n\nI cihe !\nD:\n\nTINToutheanere thx.\nNourm nd anoseshad thood\nF tounds t and, am, lofoupit qur ime foung st ns sishout, d In cof, tat tolll mpitorised??\n\nAnd arruldoid d ve, o wsetorco le RDid.\nFifr wentooy ween:\nWhay, that E chald\n\n\nAfter training, the generated text looks more like human writing, but the words still do not make any sense. However, we have made progress."
  },
  {
    "objectID": "posts/adventure/holi/index.html",
    "href": "posts/adventure/holi/index.html",
    "title": "Awesome Holi Day",
    "section": "",
    "text": "Wow, I was totally caught off guard but it turned out to be one of the best days ever! It was a wild day full of music, food, colors, water, and mud. We all let loose and acted like kids again, forgetting about work for a while. I danced my heart out, following my friends’ moves and even throwing in some of my own sweet grooves. It got so crazy that my friends even tore my shirt and dragged me into the mud!\nSince there wasn’t any dinner available in the mess, we had a blast eating pizza and burgers while chatting it up under the gorgeous moonlight. To cap off the day, we all settled in for a movie night and watched “Phir Hera Pheri.” It was the perfect way to end a perfect day.\n\n\n\n\n\n\nSuchith\n\n\n\n\n\n\n\nMe again\n\n\n\n\n\n\n\n\n\n\n\nSIT Gang\n\n\n\n\n\n\n\nPeople playing with Mud\n\n\n\n\n\n\n\n\n\n\n\nAdi, Nikita, Megha and Us\n\n\n\n\n\n\n\nShivani, Suchith, Mohit, Mridul\n\n\n\n\n\n\n\n\n\n\n\nSuchith, Mridul and Mohit\n\n\n\n\n\n\n\nMovie Night"
  },
  {
    "objectID": "posts/adventure/pusa-debacle/index.html",
    "href": "posts/adventure/pusa-debacle/index.html",
    "title": "Pusa Debacle",
    "section": "",
    "text": "Pusa in Delhi is a really beautiful place, with vast farm lands in the midst of the concrete jungle. If you want to experience the village vibe in Delhi, a visit to the IARI and ICAR campus is a must. The greenery there is truly amazing, and you wouldn’t believe you’re in a metropolitan city. Not only that, but the variety of plants grown there will enhance your plant identification skills. The beautiful big trees and single-storey houses with vast garden areas will leave you in awe.\nUnfortunately, our first attempt to visit Pusa was unsuccessful as the G20 summit was being held on the same day. But we didn’t give up, and embarked on the 30 km journey again. However, this trip turned out to be a nightmare, with an unexpected heartbreak. It’s ironic that such a beautiful place could also be the setting of a difficult experience. But that’s life, right? We have to keep striving and look forward to another trip to make up for the sour experience.\n\n\n\n\n\n\nMy wonder bag\n\n\n\n\n\n\n\nSuchith\n\n\n\n\n\n\n\n\n\n\n\nPink Dahlia\n\n\n\n\n\n\n\nWhite Dahlia\n\n\n\n\n\n\n\nDahlias\n\n\n\n\n\n\n\n\nCarrot\n\n\n\n\n\n\n\n\nCauliflower\n\n\n\n\n\n\n\nOnion\n\n\n\n\n\n\n\n\n\n\n\nPusa road\n\n\n\n\n\n\n\nDurga temple\n\n\n\n\n\n\n\nDurga temple\n\n\n\n\n\n\n\n\nBack to IIT"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Wondering Wanderer",
    "section": "",
    "text": "“Life is a journey, and if you fall in love with the journey, you will be in love forever.” - Peter Hagerty\n\n\n\n\n\n\nHi, I am your King Suchith, and welcome you all to Utopia — a world guided by kindness, fairness, and unity. Here, people support one another, value equality and respect, and work together for shared growth. Education, creativity, and personal development are encouraged, and harmony with nature and society ensures that happiness is not just an aim, but a way of life.\n\nBeyond ruling Utopia and guiding its vision, I am a PhD scholar working on extreme classification, exploring the frontiers of machine learning for real-world impact. In my free time, I enjoy painting, playing tennis, and practicing the tabla—an art form close to my heart. Through this website, I share my interests in research, art, music, and movement, and invite you to join me on a journey of learning, creativity, and discovery.",
    "crumbs": [
      "Home",
      "Suchith Prabhu",
      "Home"
    ]
  },
  {
    "objectID": "adventure_blog.html",
    "href": "adventure_blog.html",
    "title": "Roaming Beyond Borders",
    "section": "",
    "text": "Awesome Holi Day\n\n\nOMG, my first ever hostel Holi was simply amazing!\n\n\n\n\n\nMar 8, 2023\n\n\nSuchith Prabhu\n\n\n\n\n\n\n\n\n\n\n\n\nPusa Debacle\n\n\nMy trip to one of the coolest places in Delhi turned out to be a complete nightmare.\n\n\n\n\n\nMar 5, 2023\n\n\nSuchith Prabhu\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Home",
      "Suchith Prabhu",
      "Blogs",
      "Adventure"
    ]
  }
]