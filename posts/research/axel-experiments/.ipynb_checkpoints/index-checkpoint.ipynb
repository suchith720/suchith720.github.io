{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a917e1a0-c282-4317-ae36-9aff099cc018",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"AXEL Dataset\"\n",
    "author: \"Suchith Prabhu\"\n",
    "date: \"2023-04-29\"\n",
    "jupyter: python3\n",
    "description: The purpose of this article is to detail the utilization of various datasets in the experimentation of the AXEL model, including how they are being used and what types of datasets are involved. \n",
    "image: \"images/axel_model.png\"\n",
    "sidebar: false\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098d2f25-9381-4db5-a738-30e8921f4fac",
   "metadata": {},
   "source": [
    "The AXEL model, which stands for Augmented eXtreme Classifiers for Efficient Language generation, represents a category of models that leverage both extreme classifiers (XC) and generative models to address specific tasks. An approach to integrating these two components involves augmenting the output of the extreme classifier and feeding it as input to the language model. The following diagram illustrates this methodology."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94745cbd-65ea-4505-9da5-5843c8cd1a59",
   "metadata": {},
   "source": [
    "![AXEL Model](images/axel_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d5901f-bde9-42e2-982b-565612047bab",
   "metadata": {},
   "source": [
    "This passage elucidates the datasets used for public experimentation of the model and outlines the creation of the train-test split for evaluating the model. The primary objective of these models is to enhance the quality of the generative model by combining it with an extreme classifier, with the anticipation that this fusion will yield improved results. Therefore, the task at hand involves comparing the performance of the generative model, the combined generative model with the extreme classifier, and assessing their respective outcomes.\n",
    "\n",
    "Three public datasets, namely ORCAS, Wikipedia pages, and Amazon product datasets, have been selected for this purpose. In alignment with the objective of enhancing the generative ability of the decoder for a given task by incorporating information derived from the extreme classifier, two datasets are required. The first dataset is used to train the decoder, which is the primary focus, while the second auxiliary dataset is used to train the extreme classifier, which augments the input of the decoder.\n",
    "\n",
    "Care must be taken to ensure that the extreme classifier task complements the generative task and does not impede the learning process of the decoder. One potential concern arises when there is substantial overlap between the datasets used to train both models. In such cases, the extreme classifier might provide the correct output to the decoder, resulting in the decoder learning nothing new and functioning merely as an identity function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976c039f-ad7e-486a-9453-3903ddb4e80d",
   "metadata": {},
   "source": [
    "# ORCAS dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563aced6-e4bb-4cf0-b40d-5654c908bee2",
   "metadata": {},
   "source": [
    "ORCAS is a dataset that revolves around query-to-document relationships and is constructed using click-logs. For additional details about the dataset's construction, further information can be accessed [here](https://suchith720.github.io/posts/research/orcas-related-queries/orcas_dataset_blog.html). When constructing the dataset using the dump, we have employed two distinct approaches. Given our ultimate aim of deploying the model at Bing-Ads, our focus lies on the query-to-query dataset, which closely aligns with the actual task of query-to-keyword matching."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf62ca53-c402-4ce0-b53f-b8dd89f1c84b",
   "metadata": {},
   "source": [
    "## Approach 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db751202-8603-453c-b0eb-6f278bb16e2c",
   "metadata": {},
   "source": [
    "Initially, we create a query-query dataset by incorporating all the two-hop neighbors associated with a given query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f5cfc2-31a8-4031-b99a-02b976043fca",
   "metadata": {},
   "source": [
    "![ORCASRelatedQuery dataset generation process](images/ORCASRelatedQuery_generation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404b03da-abee-46cf-ad45-fee2cf2d562f",
   "metadata": {},
   "source": [
    "Subsequently, we divide the dataset into three sections and eliminate any intersecting edges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b301eeb2-75d1-4ade-90c8-ea8f4610b848",
   "metadata": {},
   "source": [
    "![ORCASRelatedQuery train-test split](images/ORCASRelatedQuery_split.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4935afb6-7f65-420e-8f0f-345b46a6ee45",
   "metadata": {},
   "source": [
    "The first partition, denoted as $D_1$, is utilized to train the decoder, which constitutes our primary objective. The second partition, $D_2$, is employed to train the XC model, while the third partition, $D_3$, is reserved for evaluating the performance of the decoder both before and after the integration of the XC augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23367819-a955-462c-8ada-6635ee87ae22",
   "metadata": {},
   "source": [
    "![ORCASRelatedQuery dataset](images/ORCASRelatedQuery.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cbdf3b-2d77-4b99-b3d9-0034e7d4e259",
   "metadata": {},
   "source": [
    "## Approach 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e4eaac-e6a8-42ba-afe0-aaa64960d00d",
   "metadata": {},
   "source": [
    "The query-document dataset is partitioned on the document side into two distinct parts, ensuring that each query is assigned to one of the partitions based on the query-document links. This division effectively separates the dataset into two sections, completely eliminating any crossover links. Subsequently, we utilize the $D_1$ partition, known as the query-query dataset, to train the decoder, while the $D_2$ partition, which comprises the query-document dataset, is used for training the XC model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bed315-6f23-4c7c-a7f9-1d4d2d8bf22f",
   "metadata": {},
   "source": [
    "![Mix of query-document and query-query dataset.](images/ORCAS_approach2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cb5350-8e98-4c17-bf60-ad3052643c41",
   "metadata": {},
   "source": [
    "# Wikipedia articles dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3105867-505c-4a7b-a735-a38208f2138d",
   "metadata": {},
   "source": [
    "Here, the decoder is trained to make predictions regarding the `category` of a given Wikipedia article. And the auxiliary XC task involves predicting the `SeeAlso` title associated with the same article"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b118c5-c805-493c-a483-16e6ddf41348",
   "metadata": {},
   "source": [
    "![Wikipedia dataset](images/wikipedia_dataset.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f02e2db-6896-4631-b1f4-3ea722fa702b",
   "metadata": {},
   "source": [
    "# Amazon Products dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67411ccd-8395-4487-b767-9f6d9f11e92c",
   "metadata": {},
   "source": [
    "The decoder in this context is designed to generate the `also_buy` product by utilizing a given product title as input. In parallel, the XC models can be trained to predict the `also_view` and `similar items` associated with the same product, based on its title as input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a3b921-fc81-4ef0-8dd6-dd551c28d646",
   "metadata": {},
   "source": [
    "![Amazon dataset](images/amazon_dataset.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
